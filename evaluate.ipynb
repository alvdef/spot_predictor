{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from model import get_model\n",
    "from dataset import SpotDataset\n",
    "from procedures import Evaluate\n",
    "from utils import get_name, setup_logging\n",
    "\n",
    "DIR = get_name()\n",
    "print(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logging(log_level=\"INFO\", log_file=DIR + \"/evaluation.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(f\"{DIR}/data/train_df.pkl\")\n",
    "val_df = pd.read_pickle(f\"{DIR}/data/val_df.pkl\")\n",
    "test_df = pd.read_pickle(f\"{DIR}/data/test_df.pkl\")\n",
    "instance_info_df = pd.read_pickle(f\"{DIR}/data/instance_info_df.pkl\")\n",
    "\n",
    "train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "print(f\"Number of different id_instances: {test_df['id_instance'].nunique()}\")\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset before model initialization to derive feature sizes\n",
    "test_dataset = SpotDataset(test_df, instance_info_df, DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass test_dataset to get_model to automatically derive feature sizes\n",
    "model = get_model(DIR, test_dataset)\n",
    "model.load()\n",
    "\n",
    "evaluator = Evaluate(model, test_dataset, DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluator.evaluate_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
